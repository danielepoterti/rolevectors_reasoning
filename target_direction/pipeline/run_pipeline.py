import asyncio
import torch
import random
import json
import os
import argparse

from pipeline.dataset.load_datasets import load_dataset_split, load_dataset

from pipeline.config import Config
from pipeline.model_utils.model_factory import construct_model_base

from pipeline.submodules.generate_directions import generate_directions
from pipeline.submodules.select_direction import select_direction, test_directions
from tqdm import tqdm

def parse_arguments():
    """
    Parse command line arguments for the pipeline.
    This function configures an argument parser with two required arguments:
        --model_path: Path to the model file.
        --role: Role to use for training.
    Returns:
        argparse.Namespace: An object with attributes 'model_path' and 'role' containing the parsed command line arguments.
    """
    parser = argparse.ArgumentParser(description="Parse model path argument.")
    parser.add_argument('--model_path', type=str, required=True, help='Path to the model')
    parser.add_argument('--role', type=str, required=True, help='Role to use for training')
    parser.add_argument('--pos', type=int, required=True, help='Pos to use for intervention')
    return parser.parse_args()

def load_and_sample_datasets(cfg):
    """
    Load and sample datasets for training and validation.
    Parameters:
        cfg (object): Configuration object with the following attributes:
            role (str): Identifier used to select the target training split (e.g., 'train_role').
            n_train (int): Number of samples to randomly select from both the target and base training datasets.
            test (str): Identifier for the target test split to be used as validation.
            n_val (int, optional): Number of validation samples to select (currently not used since the entire test split is returned).
    Returns:
        tuple:
            - target_train (list): Randomly sampled list of training samples from the target dataset.
            - base_train (list): Randomly sampled list of training samples from the base dataset.
            - target_val (list): List of samples from the target test split used for validation.
    """
    random.seed(42)
    target_train = random.sample(load_dataset_split(datavar='target', split=f'train_{cfg.role}', instructions_only=True), cfg.n_train)
    base_train = random.sample(load_dataset_split(datavar='base', split='train', instructions_only=True), cfg.n_train)
    target_val = random.sample(load_dataset_split(datavar='target', split=f'test_{cfg.test}', instructions_only=False), cfg.n_test)
    return target_train, base_train, target_val


def generate_and_save_candidate_directions(cfg, model_base, target_train, base_train):
    """
    Generate candidate direction vectors using the provided model and training datasets,
    save the resulting differences to disk, and return the generated candidate directions.
    Parameters:
        cfg: Configuration object that provides the artifact directory path and role information.
             It should have a method artifact_path() and an attribute role used for directory naming.
        model_base: The base model used for generating candidate directions.
        target_train: Training data corresponding to the target domain.
        base_train: Training data corresponding to the base domain.
    Returns:
        mean_diffs: The candidate direction vectors (mean differences) generated by the model.
    Side Effects:
        - Creates the directory for storing candidate directions if it does not already exist.
        - Saves the generated candidate directions to a file named 'mean_diffs.pt' using torch.save.
    """
    
    if not os.path.exists(os.path.join(cfg.artifact_path(), cfg.role, 'generate_directions')):
        os.makedirs(os.path.join(cfg.artifact_path(), cfg.role, 'generate_directions'))

    mean_diffs = generate_directions(
        model_base,
        target_train,
        base_train,
        artifact_dir=os.path.join(cfg.artifact_path(), cfg.role, "generate_directions"))

    torch.save(mean_diffs, os.path.join(cfg.artifact_path(), cfg.role, 'generate_directions/mean_diffs.pt'))

    return mean_diffs

async def select_and_save_direction(cfg, model_base, target_val, candidate_directions):
    """
    Asynchronously tests candidate directions and selects the optimal direction for a given model.
    This function first creates necessary directories for storing artifacts if they do not exist.
    It then tests the candidate directions by calling an asynchronous `test_directions` function.
    If metadata about a previously selected direction exists, it loads and returns this metadata and the saved direction.
    Otherwise, it computes the optimal direction by calling `select_direction`, saves both the direction and its metadata,
    and then returns the selection details.
    Args:
        cfg: Configuration object that provides methods and attributes for artifact paths, role, test identifiers,
             coefficient values, and batch size.
        model_base: Base model (e.g., a neural network) used to evaluate or apply candidate directions.
        target_val: The target value used to assess the suitability of candidate directions.
        candidate_directions: A collection of candidate directions to be evaluated.
    Returns:
        tuple: A tuple containing:
            - pos: The position index of the selected direction.
            - layer: The layer index associated with the selected direction.
            - direction: The computed direction (typically a torch.Tensor).
    Notes:
        - The function uses `os.makedirs` to create directories if they do not exist.
        - The function expects paths returned by `cfg.artifact_path()`, `cfg.role`, `cfg.test`, and `cfg.coeff` to be valid.
        - It uses asynchronous testing with `await test_directions(...)` before selecting the direction.
    """
    
    select_dir = os.path.join(cfg.artifact_path(), cfg.role, cfg.test, str(cfg.coeff), 'select_direction')
    if not os.path.exists(select_dir):
        os.makedirs(select_dir)
    
    # print("Testing directions...")
    # await test_directions(model_base, candidate_directions, artifact_dir=os.path.join(cfg.artifact_path(), cfg.role, "test_direction", str(cfg.coeff)), cfg=cfg)
    
    metadata_file = os.path.join(cfg.artifact_path(), cfg.role, cfg.test, str(cfg.coeff), "select_direction", str(cfg.pos), "31", f"results_addition_{cfg.pos}_31.json")
    if os.path.exists(metadata_file):
        print(f"Already calculated for {cfg.pos} and 31")  
        return None, None, None

    pos, layer, direction = select_direction(
        model_base,
        target_val,
        candidate_directions,
        cfg=cfg,
        artifact_dir=os.path.join(cfg.artifact_path(), cfg.role, cfg.test, str(cfg.coeff), "select_direction"),
        batch_size=cfg.batch
    )

    if not os.path.exists(f'{cfg.artifact_path()}/{cfg.role}/{cfg.test}/{cfg.coeff}'):
        os.makedirs(f'{cfg.artifact_path()}/{cfg.role}/{cfg.test}/{cfg.coeff}')

    with open(f'{cfg.artifact_path()}/{cfg.role}/{cfg.test}/{cfg.coeff}/direction_metadata.json', "w") as f:
        json.dump({"pos": pos, "layer": layer}, f, indent=4)

    torch.save(direction, f'{cfg.artifact_path()}/{cfg.role}/{cfg.test}/{cfg.coeff}/direction.pt')

    return pos, layer, direction

def generate_and_save_completions_for_dataset(cfg, model_base, fwd_pre_hooks, fwd_hooks, intervention_label, dataset_name, dataset=None):
    """
    Generates and saves completions for a given dataset using the specified model and hooks.
    Args:
        cfg: Configuration object containing settings such as artifact path and max new tokens.
        model_base: The base model used to generate completions.
        fwd_pre_hooks: Forward pre-hooks to be applied during the generation process.
        fwd_hooks: Forward hooks to be applied during the generation process.
        intervention_label: Label used to identify the intervention in the saved completions file.
        dataset_name: Name of the dataset to load if dataset is not provided.
        dataset (optional): Pre-loaded dataset to use for generating completions. If None, the dataset will be loaded using the dataset_name.
    Returns:
        None
    """
    if not os.path.exists(os.path.join(cfg.artifact_path(), 'completions')):
        os.makedirs(os.path.join(cfg.artifact_path(), 'completions'))

    if dataset is None:
        dataset = load_dataset(dataset_name)

    completions = model_base.generate_completions(dataset[:10], fwd_pre_hooks=fwd_pre_hooks, fwd_hooks=fwd_hooks, max_new_tokens=cfg.max_new_tokens)
    
    with open(f'{cfg.artifact_path()}/completions/{dataset_name}_{intervention_label}_completions.json', "w") as f:
        json.dump(completions, f, indent=4)

def evaluate_completions_and_save_results_for_dataset(cfg, intervention_label, dataset_name, eval_methodologies):
    """
    Evaluates completions for a given dataset and saves the evaluation results to a file.
    This function reads a JSON file containing completions based on the specified dataset name
    and intervention label, evaluates these completions using the provided evaluation methodologies
    via the 'evaluate_jailbreak' function, and writes the resulting evaluation metrics back to a JSON file.
    Args:
        cfg (object): Configuration object that provides the 'artifact_path' method to locate files.
        intervention_label (str): The label specifying the intervention used in the evaluation.
        dataset_name (str): The name of the dataset for which completions are evaluated.
        eval_methodologies (list): A list of methodologies to be used for evaluating the completions.
    Returns:
        None
    Side Effects:
        Reads from and writes to JSON files located in a directory defined by 'cfg.artifact_path()'.
    """
    with open(os.path.join(cfg.artifact_path(), f'completions/{dataset_name}_{intervention_label}_completions.json'), 'r') as f:
        completions = json.load(f)

    evaluation = evaluate_jailbreak(
        completions=completions,
        methodologies=eval_methodologies,
        evaluation_path=os.path.join(cfg.artifact_path(), "completions", f"{dataset_name}_{intervention_label}_evaluations.json"),
    )

    with open(f'{cfg.artifact_path()}/completions/{dataset_name}_{intervention_label}_evaluations.json', "w") as f:
        json.dump(evaluation, f, indent=4)

def evaluate_loss_for_datasets(cfg, model_base, fwd_pre_hooks, fwd_hooks, intervention_label):
    """
    Evaluates the loss for datasets and saves the evaluation results to a JSON file.
    This function performs the following steps:
    1. Ensures that the directory for loss evaluations exists under the configured artifact path.
    2. Generates a file path for on-distribution completions using the 'harmless_baseline_completions.json' file.
    3. Calls the `evaluate_loss` function with the provided model, forward hooks, batch size, and number of batches to compute loss evaluations.
    4. Saves the computed loss evaluations to a JSON file with a name based on the provided intervention label.
    Parameters:
        cfg: An object containing configuration settings. It must have:
             - artifact_path(): a method that returns the base directory for storing artifacts,
             - ce_loss_batch_size: an integer specifying the batch size for loss evaluation,
             - ce_loss_n_batches: an integer specifying the number of batches for loss evaluation.
        model_base: The base model on which loss evaluation is performed.
        fwd_pre_hooks: Pre-forward hooks to be executed before the model's forward pass.
        fwd_hooks: Forward hooks to be executed during the model's forward pass.
        intervention_label: A string label used to name the output loss evaluation JSON file.
    Returns:
        None
    Side Effects:
        Writes the loss evaluation results to a JSON file located at:
            {cfg.artifact_path()}/loss_evals/{intervention_label}_loss_eval.json
    """
    if not os.path.exists(os.path.join(cfg.artifact_path(), 'loss_evals')):
        os.makedirs(os.path.join(cfg.artifact_path(), 'loss_evals'))

    on_distribution_completions_file_path = os.path.join(cfg.artifact_path(), f'completions/harmless_baseline_completions.json')

    loss_evals = evaluate_loss(model_base, fwd_pre_hooks, fwd_hooks, batch_size=cfg.ce_loss_batch_size, n_batches=cfg.ce_loss_n_batches, completions_file_path=on_distribution_completions_file_path)

    with open(f'{cfg.artifact_path()}/loss_evals/{intervention_label}_loss_eval.json', "w") as f:
        json.dump(loss_evals, f, indent=4)

async def run_pipeline(model_path, role, pos):
    """
    Execute the pipeline process for generating and selecting candidate refusal directions for various tests and coefficient values.
    This asynchronous function performs the following major steps:
    1. Constructs a configuration object using the provided model path and role.
    2. Loads the model's base components.
    3. Iterates over a predefined set of tests and associated coefficient values.
        - For each test and coefficient combination:
          a. Loads and samples the required datasets.
          b. Generates candidate refusal directions.
          c. Selects and saves the most effective refusal direction based on the validation dataset.
    4. Uses a progress bar (via tqdm) to track the processing progress across all test and coefficient combinations.
    Parameters:
         model_path (str): The file path to the model directory.
         role (str): The role configuration to be applied in the pipeline.
    Returns:
         None
    Raises:
         Exceptions that occur during dataset loading, candidate generation, or direction selection may propagate.
    """
    
    model_alias = os.path.basename(model_path)
    cfg = Config(model_alias=model_alias, model_path=model_path)
    model_base = construct_model_base(cfg.model_path)
    # tests = ["natural_science", "law", "econ", "eecs", "math", "medicine", "politics", "psychology"]
    # coeffs = [+1.0, +3.0]
    coeffs = [1.0]
    tests = ["math"]
    
    cfg.pos = pos
    cfg.role =  role
    
    progress_bar = tqdm(total=len(tests) * len(coeffs), desc="Processing tests")

    for test in tests:
        for coeff in coeffs:
            progress_bar.set_postfix({'role': cfg.role, 'test': test, 'coeff': coeff})
            cfg.test = test
            cfg.coeff = coeff
           
            # Load and sample datasets
            target_train, base_train, target_val = load_and_sample_datasets(cfg)

            # 1. Generate candidate refusal directions
            # It saves candidate directions (mean diffs) with the shape of (pos layer d_model)
            candidate_directions = generate_and_save_candidate_directions(cfg, model_base, target_train, base_train)

            # 2. Select the most effective refusal direction 
            pos, layer, direction = await select_and_save_direction(cfg, model_base, target_val, candidate_directions)
            
            progress_bar.update(1)
    
    progress_bar.close()

def main():
    """
    Main entry point for the pipeline execution.

    This function parses command-line arguments using `parse_arguments()`, and then initiates the pipeline by executing
    `run_pipeline()` asynchronously with the model path and role provided in the parsed arguments.

    No return value.
    """
    args = parse_arguments()
    asyncio.run(run_pipeline(model_path=args.model_path, role=args.role, pos=args.pos))

if __name__ == "__main__":
    main()