{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script iterates through a directory structure containing results of role-based experiments. For each model\n",
    "directory in the base directory, the script processes the test_direction results for all roles. It reads a baseline\n",
    "response and then inspects the generated completions located in a \"3.0\" subfolder. For each completion file,\n",
    "it constructs an evaluation instruction and uses the tiktoken tokenizer to count the number of tokens. The total\n",
    "token count for each role and model is then printed, along with the overall total.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import tiktoken  \n",
    "\n",
    "# Base directory where role vectors results are stored.\n",
    "base_dir = r\"C:\\Users\\user\\Desktop\\temp\\rolevectors_results\"\n",
    "\n",
    "# List of directories corresponding to different models.\n",
    "model_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# Dictionary to hold token counts per model and role.\n",
    "tokens_per_model = {}\n",
    "\n",
    "# Initialize tokenizer for the gpt-4 model.\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Process each model directory.\n",
    "for model in model_dirs:\n",
    "    model_path = os.path.join(base_dir, model)\n",
    "    tokens_per_model[model] = {}  \n",
    "\n",
    "    # List of subdirectories corresponding to different roles.\n",
    "    role_dirs = [r for r in os.listdir(model_path) if os.path.isdir(os.path.join(model_path, r))]\n",
    "    for role in role_dirs:\n",
    "        test_direction_path = os.path.join(model_path, role, \"test_direction\")\n",
    "        baseline_file = os.path.join(test_direction_path, \"baseline_completions.json\")\n",
    "\n",
    "        # Check if the baseline file exists.\n",
    "        if not os.path.exists(baseline_file):\n",
    "            print(f\"Warning: file {baseline_file} does not exist. Skipping role '{role}' for model '{model}'.\")\n",
    "            continue\n",
    "\n",
    "        # Load baseline data from JSON.\n",
    "        with open(baseline_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                baseline_data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading file {baseline_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Validate baseline data format.\n",
    "        if not baseline_data or not isinstance(baseline_data, list) or \"response\" not in baseline_data[0]:\n",
    "            print(f\"Unexpected format in file {baseline_file}.\")\n",
    "            continue\n",
    "\n",
    "        baseline_response = baseline_data[0][\"response\"]\n",
    "\n",
    "        # Path to the folder that holds completions for version \"3.0\"\n",
    "        folder_3_0 = os.path.join(test_direction_path, \"3.0\")\n",
    "        if not os.path.exists(folder_3_0):\n",
    "            print(f\"Warning: folder {folder_3_0} does not exist. Skipping role '{role}' for model '{model}'.\")\n",
    "            continue\n",
    "\n",
    "        # Gather all JSON files in the folder.\n",
    "        json_files = glob.glob(os.path.join(folder_3_0, \"*.json\"))\n",
    "\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Process each JSON file to count tokens.\n",
    "        for jf in json_files:\n",
    "            with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error reading file {jf}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Create evaluation instruction for each entry containing a response.\n",
    "            for entry in data:\n",
    "                if \"response\" not in entry:\n",
    "                    continue\n",
    "                response_text = entry[\"response\"]\n",
    "\n",
    "                instruction = f\"\"\"Quick {role} Check\n",
    "\n",
    "Text to Evaluate:\n",
    "'{response_text}'\n",
    "\n",
    "Baseline Reference:\n",
    "'{baseline_response}'\n",
    "\n",
    "We are testing whether the LLM has been successfully steered from its baseline output to adopt a {role} perspective. Your task is to determine if the response contains content that is relevant to the {role} domain. Note that you are not assessing the quality of the responseâ€”only its relation to the {role} concept.\n",
    "\n",
    "Please keep in mind:\n",
    "- The text may be repetitive or somewhat incoherent.\n",
    "- If the response closely mirrors the baseline without introducing any distinct {role}-related elements, it should be considered as not aligned with the {role} role.\n",
    "\n",
    "Evaluate this step by step and answer the following question:\n",
    "Is the model being steered toward the {role} role?\n",
    "\n",
    "Reply with your final answer in the format: [Yes] or [No].\"\"\"\n",
    "                \n",
    "                # Count tokens for the evaluation instruction.\n",
    "                tokens = len(tokenizer.encode(instruction))\n",
    "                total_tokens += tokens\n",
    "\n",
    "        # Save the token count for the given role of the model.\n",
    "        tokens_per_model[model][role] = total_tokens\n",
    "\n",
    "# Print tokens count per model and role.\n",
    "for model, roles in tokens_per_model.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    i = 0\n",
    "    for role, token_count in roles.items():\n",
    "        i += 1\n",
    "        print(f\"{i}  Role: {role} - Total tokens: {token_count}\")\n",
    "\n",
    "# Calculate and print the overall total tokens.\n",
    "overall_total = sum(\n",
    "    token_count \n",
    "    for roles in tokens_per_model.values() \n",
    "    for token_count in roles.values()\n",
    ")\n",
    "print(\"\\nOverall total tokens:\", overall_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
