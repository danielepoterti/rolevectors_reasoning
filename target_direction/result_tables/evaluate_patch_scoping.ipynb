{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class Generator():\n",
    "\n",
    "    def __init__(self, model: str, openrouter_key: str, providers: list, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initializes the generator with the specified model, OpenRouter key, temperature, and providers.\n",
    "        Args:\n",
    "            model (str): The name or path of the pre-trained model to use.\n",
    "            openrouter_key (str): The API key for OpenRouter.\n",
    "            temperature (float, optional): The temperature to use for sampling. Defaults to 0.9.\n",
    "            providers (list, optional): A list of providers to use. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.openrouter_key = openrouter_key\n",
    "        self.temperature = temperature\n",
    "        self.providers = providers if providers else []\n",
    "    \n",
    "    async def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronously generates a response based on the given prompt using the specified model.\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while out is None:\n",
    "                payload = json.dumps({\n",
    "                    'model': self.model,\n",
    "                    'providers': {\"order\": self.providers},\n",
    "                    'temperature': self.temperature,\n",
    "                    'messages': [\n",
    "                        {\n",
    "                            'role': 'user',\n",
    "                            'content': prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                })\n",
    "\n",
    "                headers = {\n",
    "                    'Authorization': f'Bearer {self.openrouter_key}'\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    url = 'https://openrouter.ai/api/v1/chat/completions'\n",
    "                    async with session.post(url, headers=headers, data=payload) as response:\n",
    "                        if response.status == 200:\n",
    "                            try:\n",
    "                                response_json = await response.json()\n",
    "                                tryout = response_json['choices'][0]['message']['content']\n",
    "                                tryout = tryout.strip().strip(\"b'\").strip()\n",
    "                                if tryout.startswith('An error occurred:') or tryout == '':\n",
    "                                    print('Error? Retrying...')\n",
    "                                    print(tryout)\n",
    "                                    await asyncio.sleep(0.5)\n",
    "                                    continue\n",
    "                                out = tryout\n",
    "                            except:\n",
    "                                out = str(await response.text())\n",
    "                        else:\n",
    "                            print(response.status)\n",
    "                            await asyncio.sleep(0.5)\n",
    "                except:\n",
    "                    print('Error in sending post request? Retrying...')\n",
    "                    await asyncio.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, model: str, openrouter_key: str, providers: list, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the generator with the model, OpenRouter key, list of providers, and temperature.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.openrouter_key = openrouter_key\n",
    "        self.temperature = temperature\n",
    "        self.providers = providers if providers else []\n",
    "    \n",
    "    async def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronously generate a response based on the given prompt.\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while out is None:\n",
    "                payload = json.dumps({\n",
    "                    'model': self.model,\n",
    "                    'providers': {\"order\": self.providers},\n",
    "                    'temperature': self.temperature,\n",
    "                    'messages': [\n",
    "                        {\n",
    "                            'role': 'user',\n",
    "                            'content': prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                })\n",
    "\n",
    "                headers = {\n",
    "                    'Authorization': f'Bearer {self.openrouter_key}'\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    url = 'https://openrouter.ai/api/v1/chat/completions'\n",
    "                    async with session.post(url, headers=headers, data=payload) as response:\n",
    "                        if response.status == 200:\n",
    "                            try:\n",
    "                                response_json = await response.json()\n",
    "                                tryout = response_json['choices'][0]['message']['content']\n",
    "                                tryout = tryout.strip().strip(\"b'\").strip()\n",
    "                                # If there is an error in the response, retry\n",
    "                                if tryout.startswith('An error occurred:') or tryout == '':\n",
    "                                    print('Error in response, retrying...')\n",
    "                                    await asyncio.sleep(0.5)\n",
    "                                    continue\n",
    "                                out = tryout\n",
    "                            except Exception as e:\n",
    "                                out = str(await response.text())\n",
    "                        else:\n",
    "                            print(\"HTTP status:\", response.status)\n",
    "                            await asyncio.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print('Error sending the request, retrying...')\n",
    "                    await asyncio.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "async def evaluate_candidate(generator: Generator, response_text: str, baseline_response: str, role: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt and repeats the request until a valid response containing [Yes] or [No] is obtained.\n",
    "    \"\"\"\n",
    "    instruction = f\"\"\"Quick {role} Check\n",
    "\n",
    "Text to Evaluate:\n",
    "'{response_text}'\n",
    "\n",
    "Baseline Reference:\n",
    "'{baseline_response}'\n",
    "\n",
    "We are testing whether the LLM has been successfully steered from its baseline output to adopt a {role} perspective. Your task is to determine if the response contains content that is relevant to the {role} domain. Note that you are not assessing the quality of the responseâ€”only its relation to the {role} concept.\n",
    "\n",
    "Please keep in mind:\n",
    "- The text may be repetitive or somewhat incoherent.\n",
    "- If the response closely mirrors the baseline without introducing any distinct {role}-related elements, it should be considered as not aligned with the {role} role.\n",
    "\n",
    "Evaluate this step by step and answer the following question:\n",
    "Is the model being steered toward the {role} role?\n",
    "\n",
    "Reply with your final answer in the format: [Yes] or [No].\n",
    "\"\"\"\n",
    "    # Retry loop until a valid response is received\n",
    "    while True:\n",
    "        completion = await generator.generate(instruction)\n",
    "        if \"[Yes]\" in completion or \"[No]\" in completion:\n",
    "            return completion\n",
    "        print(\"Invalid response, retrying...\")\n",
    "\n",
    "\n",
    "async def process_candidate_file(file_path: str, baseline_response: str, role: str, generator: Generator):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assuming the file contains a list of dictionaries (completions)\n",
    "    modified = False\n",
    "    for item in data:\n",
    "        # If the fields \"passed\" or \"completion\" are \"Error\", proceed with evaluation\n",
    "        if item.get(\"passed\") == \"Error\" or item.get(\"completion\") == \"Error\":\n",
    "            response_text = item.get(\"response\", \"\")\n",
    "            evaluation = await evaluate_candidate(generator, response_text, baseline_response, role)\n",
    "            # Update fields based on the evaluation response\n",
    "            if \"[Yes]\" in evaluation:\n",
    "                item[\"passed\"] = \"Yes\"\n",
    "            elif \"[No]\" in evaluation:\n",
    "                item[\"passed\"] = \"No\"\n",
    "            item[\"completion\"] = evaluation\n",
    "            modified = True\n",
    "        else:\n",
    "            # If there are no errors, the file has already been processed\n",
    "            print(f\"The file {file_path} is already processed, skipping.\")\n",
    "\n",
    "    # Overwrite the file if modifications were made\n",
    "    if modified:\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"File {file_path} updated.\")\n",
    "    else:\n",
    "        print(f\"No changes for file {file_path}.\")\n",
    "\n",
    "\n",
    "async def process_role(model_path: str, role: str, generator: Generator):\n",
    "    print(f\"Processing role: {role} for model: {model_path}\")\n",
    "    role_path = os.path.join(model_path, role)\n",
    "    test_direction_path = os.path.join(role_path, \"test_direction\")\n",
    "    baseline_file = os.path.join(test_direction_path, \"baseline_completions.json\")\n",
    "    \n",
    "    if not os.path.exists(baseline_file):\n",
    "        print(f\"Baseline file not found for role '{role}' in {model_path}.\")\n",
    "        return\n",
    "\n",
    "    # Load the baseline file\n",
    "    try:\n",
    "        with open(baseline_file, \"r\") as f:\n",
    "            baseline_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading baseline file {baseline_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Use the first element as baseline reference; adjust as needed\n",
    "    if len(baseline_data) == 0:\n",
    "        print(f\"Baseline file is empty for role '{role}' in {model_path}.\")\n",
    "        return\n",
    "    baseline_response = baseline_data[0].get(\"response\", \"\")\n",
    "\n",
    "    # Path to folder 3.0\n",
    "    dir_3_0 = os.path.join(test_direction_path, \"3.0\")\n",
    "    if not os.path.isdir(dir_3_0):\n",
    "        print(f\"Folder '3.0' not found for role '{role}' in {model_path}.\")\n",
    "        return\n",
    "\n",
    "    # Find all JSON files in folder 3.0\n",
    "    candidate_files = [os.path.join(dir_3_0, f) for f in os.listdir(dir_3_0) if f.endswith(\".json\")]\n",
    "    tasks = []\n",
    "    for file_path in candidate_files:\n",
    "        tasks.append(process_candidate_file(file_path, baseline_response, role, generator))\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n",
    "    else:\n",
    "        print(f\"No candidate file found in {dir_3_0} for role '{role}'.\")\n",
    "\n",
    "\n",
    "async def process_model(model_path: str, generator: Generator):\n",
    "    print(f\"\\nProcessing model: {model_path}\")\n",
    "    # Get all subdirectories of the model (considered roles)\n",
    "    roles = [d for d in os.listdir(model_path) if os.path.isdir(os.path.join(model_path, d))]\n",
    "    if not roles:\n",
    "        print(f\"No roles found in {model_path}.\")\n",
    "        return\n",
    "\n",
    "    # Process roles sequentially\n",
    "    for role in roles:\n",
    "        await process_role(model_path, role, generator)\n",
    "    print(f\"Processing model {model_path} completed.\")\n",
    "\n",
    "\n",
    "async def main(models_root: str):\n",
    "    # Generator configuration: replace with the correct values\n",
    "    model_test = \"anthropic/claude-3.5-haiku\"  # or the model of your choice\n",
    "    openrouter_key = os.getenv(\"OPENROUTER_KEY\")\n",
    "    if not openrouter_key:\n",
    "        print(\"Error: OPENROUTER_KEY has not been set in the .env file\")\n",
    "        return\n",
    "\n",
    "    providers_test = [\"Amazon Bedrock\", \"Anthropic\"]\n",
    "    temperature_test = 0\n",
    "\n",
    "    generator = Generator(\n",
    "        model=model_test, \n",
    "        openrouter_key=openrouter_key, \n",
    "        providers=providers_test, \n",
    "        temperature=temperature_test\n",
    "    )\n",
    "\n",
    "    # Find all model directories in the models_root\n",
    "    model_dirs = [os.path.join(models_root, d) for d in os.listdir(models_root)\n",
    "                  if os.path.isdir(os.path.join(models_root, d))]\n",
    "    \n",
    "    # Process models sequentially with a loading bar\n",
    "    for model_path in tqdm(model_dirs, desc=\"Processing models\"):\n",
    "        await process_model(model_path, generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_root = r\"C:\\Users\\user\\Desktop\\temp\\rolevectors_results\"  \n",
    "\n",
    "\n",
    "await main(models_root)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
