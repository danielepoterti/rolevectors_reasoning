{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\user\\Desktop\\temp\\rolevectors_results\\Qwen-7B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_DATASET_MAPPING = {\n",
    "    \"econ\": [\"economic researcher\", \"economist\", \"financial analyst\"],\n",
    "    \"eecs\": [\"electronics technician\", \"data scientist\", \"electrical engineer\", \"software engineer\", \"web developer\"],\n",
    "    \"law\": [\"bailiff\", \"lawyer\"],\n",
    "    \"math\": [\"data analyst\", \"mathematician\", \"statistician\"],\n",
    "    \"medicine\": [\"nurse\", \"doctor\", \"physician\", \"dentist\", \"surgeon\"],\n",
    "    \"natural science\": [\"geneticist\", \"biologist\", \"physicist\", \"teacher\", \"chemist\", \"ecologist\"],\n",
    "    \"politics\": [\"politician\", \"sheriff\", \"governor\", \"enthusiast\", \"partisan\"],\n",
    "    \"psychology\": [\"psychologist\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_baseline(df):\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_formatted = df.copy()\n",
    "    \n",
    "    # For each dataset in the MultiIndex (first level of columns)\n",
    "    for dataset in df.columns.levels[0]:\n",
    "        if 'baseline' in df[dataset].columns:\n",
    "            # Instead of rounding, simply convert to string.\n",
    "            df_formatted[(dataset, 'baseline')] = df[dataset]['baseline'].apply(lambda x: str(x))\n",
    "    \n",
    "    return df_formatted\n",
    "\n",
    "def latex_color_columns(df):\n",
    "    \"\"\"\n",
    "    Returns strings that include LaTeX commands (e.g. \\cellcolor from the xcolor package)\n",
    "    and wraps numerical values in math mode. For columns representing percentual increments \n",
    "    (e.g., \"1.0\", \"3.0\", \"ablation\"), the cell is colored only if the row’s role (assumed to be\n",
    "    the row index) is associated with the dataset according to ROLE_DATASET_MAPPING.\n",
    "    \"\"\"\n",
    "    \n",
    "    def latex_color_value(val, _baseline):\n",
    "        \"\"\"\n",
    "        Formats a value (which is already the percentual increment) with a LaTeX cell color command.\n",
    "        Positive increments are colored in green, negative in red, with intensity based on the magnitude.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(val, str) and '±' in val:\n",
    "                # Extract the main value, ignoring the uncertainty part\n",
    "                val = float(val.split('±')[0].strip())\n",
    "            else:\n",
    "                val = float(val)\n",
    "        except Exception:\n",
    "            val = 0.0\n",
    "\n",
    "        # Round to one decimal; here, val represents the percentual increment.\n",
    "        val = round(val, 1)\n",
    "        \n",
    "        # Compute a color command only if there is an increment.\n",
    "        if val == 0:\n",
    "            color_cmd = \"\"\n",
    "        else:\n",
    "            # Logarithmic scaling to determine intensity.\n",
    "            intensity_value = np.log1p(abs(val))  # log(1 + |percentage increment|)\n",
    "            factor = 10  # scaling factor (adjust as needed)\n",
    "            min_pct = 10\n",
    "            max_pct = 50\n",
    "            pct = int(np.clip(intensity_value * factor, min_pct, max_pct))\n",
    "            color_cmd = f\"\\\\cellcolor{{green!{pct}}}\" if val > 0 else f\"\\\\cellcolor{{red!{pct}}}\"\n",
    "        \n",
    "        # Force a plus sign for non-negative values.\n",
    "        if val >= 0:\n",
    "            formatted_val = f\"+{val:.1f}\\\\%\"\n",
    "        else:\n",
    "            formatted_val = f\"{val:.1f}\\\\%\"\n",
    "        \n",
    "        return f\"{color_cmd} $ {formatted_val} $\"\n",
    "    \n",
    "    def plain_format_value(val):\n",
    "        \"\"\"\n",
    "        Formats the value without applying any cell color command.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(val, str) and '±' in val:\n",
    "                val = float(val.split('±')[0].strip())\n",
    "            else:\n",
    "                val = float(val)\n",
    "        except Exception:\n",
    "            val = 0.0\n",
    "        val = round(val, 1)\n",
    "        formatted_val = f\"+{val:.1f}\\\\%\" if val >= 0 else f\"{val:.1f}\\\\%\"\n",
    "        return f\"$ {formatted_val} $\"\n",
    "    \n",
    "    def latex_baseline(val):\n",
    "        # Wrap the baseline value in math mode (preserving its original precision)\n",
    "        return f\"$ {val} $\"\n",
    "    \n",
    "    def style_df(x):\n",
    "        # Create an empty DataFrame for the LaTeX-formatted cell contents.\n",
    "        df_styled = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "        \n",
    "        # Iterate over datasets (first level of columns).\n",
    "        for dataset in x.columns.levels[0]:\n",
    "            if 'baseline' in x[dataset].columns:\n",
    "                baseline_values = x[dataset]['baseline']\n",
    "                for idx in x.index:\n",
    "                    # Always format the baseline column.\n",
    "                    df_styled.loc[idx, (dataset, 'baseline')] = latex_baseline(x.loc[idx, (dataset, 'baseline')])\n",
    "                    # Assume the row index is the role (case-insensitive).\n",
    "                    role = str(idx).lower()\n",
    "                    allowed_roles = [r.lower() for r in ROLE_DATASET_MAPPING.get(dataset, [])]\n",
    "                    \n",
    "                    # Process other columns (e.g., \"1.0\", \"3.0\", \"ablation\").\n",
    "                    for col in x[dataset].columns:\n",
    "                        if col == 'baseline':\n",
    "                            continue\n",
    "                        value = x.loc[idx, (dataset, col)]\n",
    "                        if role in allowed_roles:\n",
    "                            df_styled.loc[idx, (dataset, col)] = latex_color_value(value, baseline_values[idx])\n",
    "                        else:\n",
    "                            df_styled.loc[idx, (dataset, col)] = plain_format_value(value)\n",
    "        return df_styled\n",
    "    \n",
    "    return style_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping from dataset category to list of roles.\n",
    "ROLE_DATASET_MAPPING = {\n",
    "    \"econ\": [\"economic researcher\", \"economist\", \"financial analyst\"],\n",
    "    \"eecs\": [\"electronics technician\", \"data scientist\", \"electrical engineer\", \"software engineer\", \"web developer\"],\n",
    "    \"law\": [\"bailiff\", \"lawyer\"],\n",
    "    \"math\": [\"data analyst\", \"mathematician\", \"statistician\"],\n",
    "    \"medicine\": [\"nurse\", \"doctor\", \"physician\", \"dentist\", \"surgeon\"],\n",
    "    \"natural_science\": [\"geneticist\", \"biologist\", \"physicist\", \"teacher\", \"chemist\", \"ecologist\"],\n",
    "    \"politics\": [\"politician\", \"sheriff\", \"governor\", \"enthusiast\", \"partisan\"],\n",
    "    \"psychology\": [\"psychologist\"]\n",
    "}\n",
    "\n",
    "def get_dataset_category(role):\n",
    "    \"\"\"\n",
    "    Returns the dataset category for a given role by looking up the mapping.\n",
    "    \"\"\"\n",
    "    for category, roles in ROLE_DATASET_MAPPING.items():\n",
    "        if role in roles:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "def compute_mean_score(filepath):\n",
    "    \"\"\"\n",
    "    Given a JSON file (a list of dicts), compute the mean of the 'score' values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        scores = [d[\"score\"] for d in data if \"score\" in d]\n",
    "        return round(np.mean(scores), 2) if scores else np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def get_baseline(select_dir):\n",
    "    \"\"\"\n",
    "    In the run version folder (e.g. \"1.0/select_direction\"),\n",
    "    load results_baseline.json and return its mean score.\n",
    "    \"\"\"\n",
    "    baseline_file = os.path.join(select_dir, \"results_baseline.json\")\n",
    "    if os.path.exists(baseline_file):\n",
    "        return compute_mean_score(baseline_file)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_addition_scores(select_dir, layer_range=False, start_percent=50):\n",
    "    \"\"\"\n",
    "    Iterate over coefficient subdirectories under select_dir and compute scores.\n",
    "    \"\"\"\n",
    "    addition_scores = []\n",
    "    for sub in os.listdir(select_dir):\n",
    "        sub_path = os.path.join(select_dir, sub)\n",
    "        if os.path.isdir(sub_path) and re.match(r\"^-?\\d+$\", sub):\n",
    "            inner_dirs = [\n",
    "                d for d in os.listdir(sub_path)\n",
    "                if os.path.isdir(os.path.join(sub_path, d)) and re.match(r\"^\\d+$\", d)\n",
    "            ]\n",
    "            inner_dirs_sorted = sorted(inner_dirs, key=lambda x: int(x))\n",
    "            n_layers = len(inner_dirs_sorted)\n",
    "            if n_layers == 0:\n",
    "                continue\n",
    "            if layer_range:\n",
    "                start_idx = int(np.floor(n_layers * (start_percent / 100)))\n",
    "                end_idx = int(np.ceil(n_layers * 0.8))\n",
    "                start_idx = min(start_idx, end_idx - 1)\n",
    "                selected_dirs = inner_dirs_sorted[start_idx:end_idx]\n",
    "            else:\n",
    "                keep_count = max(1, int(np.ceil(n_layers * 0.8)))\n",
    "                selected_dirs = inner_dirs_sorted[:keep_count]\n",
    "            \n",
    "            for inner in selected_dirs:\n",
    "                inner_path = os.path.join(sub_path, inner)\n",
    "                for file in os.listdir(inner_path):\n",
    "                    if file.startswith(\"results_addition\") and file.endswith(\".json\"):\n",
    "                        file_path = os.path.join(inner_path, file)\n",
    "                        mean_file_score = compute_mean_score(file_path)\n",
    "                        if not np.isnan(mean_file_score):\n",
    "                            addition_scores.append(mean_file_score)\n",
    "    \n",
    "    if addition_scores:\n",
    "        return np.mean(addition_scores), np.std(addition_scores)\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def get_select_best_addition_score(version_path):\n",
    "    \"\"\"\n",
    "    Get addition score for select_best mode by reading the metadata file.\n",
    "    The metadata is expected at:\n",
    "      {model}/{role}/{dataset}/{coeff}/direction_metadata.json\n",
    "    Then, the addition score is computed from:\n",
    "      {model}/{role}/{dataset}/{coeff}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(version_path, \"direction_metadata.json\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"Warning: metadata file not found in {metadata_path}\")\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metadata file {metadata_path}: {e}\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    if \"pos\" not in metadata or \"layer\" not in metadata:\n",
    "        print(f\"Warning: metadata file {metadata_path} missing 'pos' or 'layer'\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    pos = str(metadata[\"pos\"])\n",
    "    layer = str(metadata[\"layer\"])\n",
    "    select_dir = os.path.join(version_path, \"select_direction\")\n",
    "    addition_file = os.path.join(select_dir, pos, layer, f\"results_addition_{pos}_{layer}.json\")\n",
    "    if not os.path.exists(addition_file):\n",
    "        print(f\"Warning: addition file not found: {addition_file}\")\n",
    "        return np.nan, np.nan\n",
    "    score = compute_mean_score(addition_file)\n",
    "    # Since this is a single performance, we return the mean only (std remains NaN)\n",
    "    return score, np.nan\n",
    "\n",
    "def process_run_version(version_path, layer_range=False, start_percent=50, select_best=False):\n",
    "    \"\"\"\n",
    "    Process the run version and compute baseline and addition scores.\n",
    "    If select_best is True, the metadata is used to select the single best direction.\n",
    "    \"\"\"\n",
    "    select_dir = os.path.join(version_path, \"select_direction\")\n",
    "    if not os.path.exists(select_dir):\n",
    "        return (np.nan, (np.nan, np.nan))\n",
    "    \n",
    "    # Only version \"1.0\" is expected to have a baseline.\n",
    "    base_val = get_baseline(select_dir) if os.path.basename(version_path) == \"1.0\" else np.nan\n",
    "    \n",
    "    if select_best:\n",
    "        addition_mean, addition_std = get_select_best_addition_score(version_path)\n",
    "    else:\n",
    "        addition_mean, addition_std = get_addition_scores(select_dir, layer_range, start_percent)\n",
    "    \n",
    "    return base_val, (addition_mean, addition_std)\n",
    "\n",
    "def get_best_direction(role, primary_dataset_path, version, baseline):\n",
    "    \"\"\"\n",
    "    For a given role and version folder inside the primary dataset folder, load the filtered direction evaluations file\n",
    "    and return (position, layer) as strings.\n",
    "\n",
    "    Expects the JSON file at:\n",
    "       {model}/{role}/{primary_dataset}/{version}/select_direction/direction_evaluations_filtered.json\n",
    "\n",
    "    When version is \"3.0\", since the file in that folder doesn't contain valid ablation information,\n",
    "    the ablation values are retrieved from the baseline file at:\n",
    "       {model}/{role}/{primary_dataset}/1.0/select_direction/direction_evaluations_filtered.json\n",
    "\n",
    "    For versions other than \"3.0\", the JSON file is expected to contain both steering and ablation values.\n",
    "    \n",
    "    The function first searches for an entry where:\n",
    "       - steering_performance_score >= baseline, and\n",
    "       - ablation_performance_score < baseline.\n",
    "       \n",
    "    If no such entry is found:\n",
    "       - For version \"3.0\": the function selects the candidate with the highest steering_performance_score from the 3.0 file.\n",
    "       - For version \"1.0\": the function relaxes the ablation condition and selects the candidate with the highest steering_performance_score.\n",
    "    \n",
    "    If a suitable entry is found, its \"position\" and \"layer\" values are returned as strings.\n",
    "    Otherwise, (None, None) is returned.\n",
    "    \"\"\"\n",
    "    if version == \"3.0\":\n",
    "        eval_file_path = os.path.join(\n",
    "            primary_dataset_path, version, \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        # Load ablation values from baseline (version \"1.0\")\n",
    "        baseline_eval_file_path = os.path.join(\n",
    "            primary_dataset_path, \"1.0\", \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(eval_file_path):\n",
    "            print(f\"Warning: filtered evaluations file not found for role '{role}' in {eval_file_path}\")\n",
    "            return None, None\n",
    "        if not os.path.exists(baseline_eval_file_path):\n",
    "            print(f\"Warning: baseline filtered evaluations file not found for role '{role}' in {baseline_eval_file_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            with open(eval_file_path, \"r\") as f:\n",
    "                eval_entries = json.load(f)\n",
    "            with open(baseline_eval_file_path, \"r\") as f:\n",
    "                baseline_entries = json.load(f)\n",
    "            \n",
    "            # Build a dictionary mapping (position, layer) to ablation_performance_score from the baseline file.\n",
    "            baseline_ablation = {}\n",
    "            for entry in baseline_entries:\n",
    "                if (\"position\" in entry and \"layer\" in entry and \n",
    "                    \"ablation_performance_score\" in entry):\n",
    "                    key = (str(entry[\"position\"]), str(entry[\"layer\"]))\n",
    "                    baseline_ablation[key] = entry[\"ablation_performance_score\"]\n",
    "            \n",
    "            # First, try to find an entry satisfying the conditions.\n",
    "            for entry in eval_entries:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    pos = str(entry[\"position\"])\n",
    "                    layer = str(entry[\"layer\"])\n",
    "                    steer_perf = entry[\"steering_performance_score\"]\n",
    "                    \n",
    "                    key = (pos, layer)\n",
    "                    if key not in baseline_ablation:\n",
    "                        continue\n",
    "                    ablation_perf = baseline_ablation[key]\n",
    "                    \n",
    "                    if steer_perf >= baseline and ablation_perf < baseline:\n",
    "                        return pos, layer\n",
    "            \n",
    "            # If no candidate meets the conditions, select the best candidate from the 3.0 evaluations\n",
    "            best_entry = None\n",
    "            best_score = -float(\"inf\")\n",
    "            for entry in eval_entries:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    score = entry[\"steering_performance_score\"]\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_entry = entry\n",
    "            \n",
    "            if best_entry is not None:\n",
    "                return str(best_entry[\"position\"]), str(best_entry[\"layer\"])\n",
    "            else:\n",
    "                print(f\"Warning: No valid entries found in {eval_file_path}.\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading evaluations for role '{role}' in version '3.0': {e}\")\n",
    "            return None, None\n",
    "\n",
    "    else:\n",
    "        # For versions other than \"3.0\"\n",
    "        eval_file_path = os.path.join(\n",
    "            primary_dataset_path, version, \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(eval_file_path):\n",
    "            print(f\"Warning: filtered evaluations file not found for role '{role}' in {eval_file_path}\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            with open(eval_file_path, \"r\") as f:\n",
    "                evaluations = json.load(f)\n",
    "                \n",
    "            # First, try to find an entry satisfying the conditions.\n",
    "            for entry in evaluations:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"ablation_performance_score\" in entry and\n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    \n",
    "                    if (entry[\"steering_performance_score\"] >= baseline and \n",
    "                        entry[\"ablation_performance_score\"] < baseline):\n",
    "                        return str(entry[\"position\"]), str(entry[\"layer\"])\n",
    "            \n",
    "            # For version \"1.0\", relax the ablation requirement if no candidate is found.\n",
    "            if version == \"1.0\":\n",
    "                best_entry = None\n",
    "                best_score = -float(\"inf\")\n",
    "                for entry in evaluations:\n",
    "                    if (\"steering_performance_score\" in entry and \n",
    "                        \"position\" in entry and \n",
    "                        \"layer\" in entry):\n",
    "                        score = entry[\"steering_performance_score\"]\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_entry = entry\n",
    "                if best_entry is not None:\n",
    "                    return str(best_entry[\"position\"]), str(best_entry[\"layer\"])\n",
    "            \n",
    "            print(f\"Warning: No evaluation entry in {eval_file_path} satisfies the conditions.\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading evaluations for role '{role}' in {eval_file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_score_for_dataset_with_direction(model_name, role, dataset, version, pos, layer):\n",
    "    \"\"\"\n",
    "    For a given role, dataset, and version (coefficient) folder, load the results file using the provided\n",
    "    best direction parameters, compute the new score from:\n",
    "       {model_name}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "    Then, load the baseline score from:\n",
    "       {model_name}/{role}/{dataset}/1.0/select_direction/results_baseline.json\n",
    "    and return the percentual increment (with sign) as a formatted string:\n",
    "       (new_score - baseline) / baseline * 100\n",
    "    If any step fails, \"nan\" is returned.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(\n",
    "        model_name, role, dataset, version, \"select_direction\", pos, layer,\n",
    "        f\"results_addition_{pos}_{layer}.json\"\n",
    "    )\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: score file not found: {file_path}\")\n",
    "        return \"nan\"\n",
    "    \n",
    "    new_score = compute_mean_score(file_path)\n",
    "    \n",
    "    # Load the baseline score from version \"1.0\".\n",
    "    baseline_dir = os.path.join(model_name, role, dataset, \"1.0\", \"select_direction\")\n",
    "    baseline_score = get_baseline(baseline_dir)\n",
    "    \n",
    "    if np.isnan(new_score) or np.isnan(baseline_score) or baseline_score == 0:\n",
    "        return \"nan\"\n",
    "    \n",
    "    increment = (new_score - baseline_score) / baseline_score * 100\n",
    "    return f\"{increment:+.1f}\"\n",
    "\n",
    "\n",
    "def get_score_for_dataset_with_direction_ablation(model_name, role, dataset, version, pos, layer):\n",
    "    \"\"\"\n",
    "    For a given role, dataset, and version (coefficient) folder, load the results file using the provided\n",
    "    best direction parameters, compute the new score from:\n",
    "       {model_name}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_ablation_{pos}_{layer}.json\n",
    "    Then, load the baseline score from:\n",
    "       {model_name}/{role}/{dataset}/1.0/select_direction/results_baseline.json\n",
    "    and return the percentual increment (with sign) as a formatted string:\n",
    "       (new_score - baseline) / baseline * 100\n",
    "    If any step fails, \"nan\" is returned.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(\n",
    "        model_name, role, dataset, version, \"select_direction\", pos, layer,\n",
    "        f\"results_ablation_{pos}_{layer}.json\"\n",
    "    )\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: score file not found: {file_path}\")\n",
    "        return \"nan\"\n",
    "    \n",
    "    new_score = compute_mean_score(file_path)\n",
    "    \n",
    "    # Load the baseline score from version \"1.0\".\n",
    "    baseline_dir = os.path.join(model_name, role, dataset, \"1.0\", \"select_direction\")\n",
    "    baseline_score = get_baseline(baseline_dir)\n",
    "    \n",
    "    if np.isnan(new_score) or np.isnan(baseline_score) or baseline_score == 0:\n",
    "        return \"nan\"\n",
    "    \n",
    "    increment = (new_score - baseline_score) / baseline_score * 100\n",
    "    return f\"{increment:+.1f}\"\n",
    "\n",
    "\n",
    "def main(model_name, layer_range=False, start_percent=50, select_best=False, select_best_role=False, select_best_role_ablation=False):\n",
    "    \"\"\"\n",
    "    Builds a MultiIndex DataFrame reporting performance scores.\n",
    "    \n",
    "    When select_best_role is True, for each role the code:\n",
    "      1. Determines the \"primary\" dataset via get_dataset_category().\n",
    "      2. In that primary folder (e.g. {model}/{role}/{primary_dataset}) identifies all version folders.\n",
    "      3. For each version, loads the best direction from its filtered evaluations file using the provided baseline.\n",
    "      4. Then for every dataset folder under {model}/{role} (excluding folders like \"generate_directions\" or \"test_direction\"),\n",
    "         if that folder contains the same version folder, the score is loaded from:\n",
    "              {model}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "      5. For version \"1.0\", the score is also recorded as the baseline.\n",
    "    \n",
    "    The DataFrame’s rows are roles and its columns use a MultiIndex with:\n",
    "      - Level 0: dataset folder name (e.g. \"law\", \"math\", …)\n",
    "      - Level 1: version (with \"baseline\" included for version \"1.0\")\n",
    "    \n",
    "    (When select_best_role is False, the original logic is used; that branch is not implemented here.)\n",
    "    \"\"\"\n",
    "    if sum([bool(layer_range), bool(select_best), bool(select_best_role), bool(select_best_role_ablation)]) > 1:\n",
    "         raise ValueError(\"Only one of layer_range, select_best, select_best_role, or select_best_role_ablation can be True.\")\n",
    "    \n",
    "    if select_best_role:\n",
    "         roles = [d for d in os.listdir(model_name) if os.path.isdir(os.path.join(model_name, d))]\n",
    "         results = {}\n",
    "         for role in roles:\n",
    "              role_path = os.path.join(model_name, role)\n",
    "              primary_dataset = get_dataset_category(role)\n",
    "              if primary_dataset is None:\n",
    "                  print(f\"Warning: No primary dataset mapping for role '{role}'. Skipping best-direction evaluation for this role.\")\n",
    "                  continue\n",
    "              primary_dataset_path = os.path.join(role_path, primary_dataset)\n",
    "              if not os.path.exists(primary_dataset_path):\n",
    "                  print(f\"Warning: Primary dataset folder '{primary_dataset_path}' not found for role '{role}'. Skipping.\")\n",
    "                  continue\n",
    "              # Compute baseline from the primary dataset's version \"1.0\" select_direction folder.\n",
    "              baseline_select_dir = os.path.join(primary_dataset_path, \"1.0\", \"select_direction\")\n",
    "              baseline_value = get_baseline(baseline_select_dir)\n",
    "              # Get the list of version folders (e.g. \"1.0\", \"3.0\", etc.) from the primary dataset folder.\n",
    "              primary_versions = [v for v in os.listdir(primary_dataset_path) if re.match(r\"^\\d+\\.\\d+$\", v)]\n",
    "              primary_versions = sorted(primary_versions, key=lambda x: float(x))\n",
    "              # Get all dataset directories under the role (excluding known non-performance folders).\n",
    "              all_datasets = [d for d in os.listdir(role_path) \n",
    "                              if os.path.isdir(os.path.join(role_path, d)) \n",
    "                              and d not in [\"generate_directions\", \"test_direction\"]]\n",
    "              # Prepare a dictionary: for each dataset, a dict mapping version (and baseline) to score.\n",
    "              ds_result = {ds: {} for ds in all_datasets}\n",
    "              for ver in primary_versions:\n",
    "                   # Get best direction using the new function that requires baseline.\n",
    "                   pos, layer = get_best_direction(role, primary_dataset_path, ver, baseline_value)\n",
    "                   if pos is None or layer is None:\n",
    "                        print(f\"Skipping version {ver} for role '{role}' due to missing best direction.\")\n",
    "                        continue\n",
    "                   # For each dataset folder, if it contains the same version folder, load its score.\n",
    "                   for ds in all_datasets:\n",
    "                        ds_ver_path = os.path.join(role_path, ds, ver)\n",
    "                        if os.path.exists(ds_ver_path):\n",
    "                            score = get_score_for_dataset_with_direction(model_name, role, ds, ver, pos, layer)\n",
    "                            ds_result[ds][ver] = score if score != \"nan\" else \"nan\"\n",
    "                        else:\n",
    "                            ds_result[ds][ver] = \"nan\"\n",
    "                        # If this is the \"1.0\" version, also record its baseline.\n",
    "                        if ver == \"1.0\":\n",
    "                            select_dir = os.path.join(model_name, role, ds, ver, \"select_direction\")\n",
    "                            baseline_score = get_baseline(select_dir)\n",
    "                            ds_result[ds][\"baseline\"] = f\"{baseline_score:.2f}\" if not np.isnan(baseline_score) else \"nan\"\n",
    "              results[role] = ds_result\n",
    "         \n",
    "         # Build a DataFrame from the results dictionary.\n",
    "         all_dataset_set = set()\n",
    "         all_version_set = set()\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   all_dataset_set.add(ds)\n",
    "                   all_version_set.update(ver_dict.keys())\n",
    "         all_dataset_list = sorted(all_dataset_set)\n",
    "         # Sort versions so that \"baseline\" always comes first, then numeric order.\n",
    "         def sort_ver(x):\n",
    "              if x == \"baseline\":\n",
    "                   return (0, 0)\n",
    "              try:\n",
    "                   return (1, float(x))\n",
    "              except:\n",
    "                   return (1, x)\n",
    "         all_version_list = sorted(list(all_version_set), key=sort_ver)\n",
    "         cols = pd.MultiIndex.from_product([all_dataset_list, all_version_list], names=[\"Dataset\", \"Version\"])\n",
    "         df = pd.DataFrame(index=sorted(results.keys()), columns=cols)\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   for ver, value in ver_dict.items():\n",
    "                        df.loc[role, (ds, ver)] = value\n",
    "         return df\n",
    "\n",
    "    elif select_best_role_ablation:\n",
    "         roles = [d for d in os.listdir(model_name) if os.path.isdir(os.path.join(model_name, d))]\n",
    "         results = {}\n",
    "         for role in roles:\n",
    "              role_path = os.path.join(model_name, role)\n",
    "              primary_dataset = get_dataset_category(role)\n",
    "              if primary_dataset is None:\n",
    "                  print(f\"Warning: No primary dataset mapping for role '{role}'. Skipping best-direction evaluation for this role.\")\n",
    "                  continue\n",
    "              primary_dataset_path = os.path.join(role_path, primary_dataset)\n",
    "              if not os.path.exists(primary_dataset_path):\n",
    "                  print(f\"Warning: Primary dataset folder '{primary_dataset_path}' not found for role '{role}'. Skipping.\")\n",
    "                  continue\n",
    "              # Compute baseline from the primary dataset's version \"1.0\" select_direction folder.\n",
    "              baseline_select_dir = os.path.join(primary_dataset_path, \"1.0\", \"select_direction\")\n",
    "              baseline_value = get_baseline(baseline_select_dir)\n",
    "              # Get the list of version folders from the primary dataset folder.\n",
    "              primary_versions = [v for v in os.listdir(primary_dataset_path) if re.match(r\"^\\d+\\.\\d+$\", v)]\n",
    "              primary_versions = sorted(primary_versions, key=lambda x: float(x))\n",
    "              # Get all dataset directories under the role (excluding known non-performance folders).\n",
    "              all_datasets = [d for d in os.listdir(role_path) \n",
    "                              if os.path.isdir(os.path.join(role_path, d)) \n",
    "                              and d not in [\"generate_directions\", \"test_direction\"]]\n",
    "              # Prepare a dictionary: for each dataset, a dict mapping version (and baseline) to score.\n",
    "              ds_result = {ds: {} for ds in all_datasets}\n",
    "              for ver in primary_versions:\n",
    "                   if ver == \"3.0\":\n",
    "                       continue\n",
    "                   # Get best direction using baseline_value.\n",
    "                   pos, layer = get_best_direction(role, primary_dataset_path, ver, baseline_value)\n",
    "                   if pos is None or layer is None:\n",
    "                        print(f\"Skipping version {ver} for role '{role}' due to missing best direction.\")\n",
    "                        continue\n",
    "                   # For each dataset folder, if it contains the same version folder, load its ablation score.\n",
    "                   for ds in all_datasets:\n",
    "                        ds_ver_path = os.path.join(role_path, ds, ver)\n",
    "                        if os.path.exists(ds_ver_path):\n",
    "                            score = get_score_for_dataset_with_direction_ablation(model_name, role, ds, ver, pos, layer)\n",
    "                            ds_result[ds][\"ablation\"] = score if score != \"nan\" else \"nan\"\n",
    "                        else:\n",
    "                            ds_result[ds][\"ablation\"] = \"nan\"\n",
    "                        # If this is the \"1.0\" version, also record its baseline.\n",
    "                        if ver == \"1.0\":\n",
    "                            select_dir = os.path.join(model_name, role, ds, ver, \"select_direction\")\n",
    "                            baseline_score = get_baseline(select_dir)\n",
    "                            ds_result[ds][\"baseline\"] = f\"{baseline_score:.2f}\" if not np.isnan(baseline_score) else \"nan\"\n",
    "              results[role] = ds_result\n",
    "         \n",
    "         # Build a DataFrame from the results dictionary.\n",
    "         all_dataset_set = set()\n",
    "         all_version_set = set()\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   all_dataset_set.add(ds)\n",
    "                   all_version_set.update(ver_dict.keys())\n",
    "         all_dataset_list = sorted(all_dataset_set)\n",
    "         # Sort versions so that \"baseline\" always comes first, then numeric order.\n",
    "         def sort_ver(x):\n",
    "              if x == \"baseline\":\n",
    "                   return (0, 0)\n",
    "              try:\n",
    "                   return (1, float(x))\n",
    "              except:\n",
    "                   return (1, x)\n",
    "         all_version_list = sorted(list(all_version_set), key=sort_ver)\n",
    "         cols = pd.MultiIndex.from_product([all_dataset_list, all_version_list], names=[\"Dataset\", \"Version\"])\n",
    "         df = pd.DataFrame(index=sorted(results.keys()), columns=cols)\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   for ver, value in ver_dict.items():\n",
    "                        df.loc[role, (ds, ver)] = value\n",
    "         return df\n",
    "\n",
    "    else:\n",
    "         raise NotImplementedError(\"Other modes (non-select_best_role) are not implemented in this snippet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_dir = model_path\n",
    "    df = main(model_dir, select_best_role_ablation=True)\n",
    "    # Print the DataFrame as LaTeX code\n",
    "    # Generate the LaTeX table code from the DataFrame\n",
    "    # Suppose df is your DataFrame with roles as its index.\n",
    "    # Assume df is your DataFrame with roles as its index.\n",
    "    desired_group_order = [\"econ\", \"eecs\", \"law\", \"math\", \"medicine\", \"natural_science\", \"politics\", \"psychology\"]\n",
    "\n",
    "    ordered_roles = []\n",
    "    for group in desired_group_order:\n",
    "        # Get roles for the group; if some roles are missing in df.index, they won't be added.\n",
    "        for role in ROLE_DATASET_MAPPING.get(group, []):\n",
    "            if role in df.index:\n",
    "                ordered_roles.append(role)\n",
    "\n",
    "    # Optionally, if there are roles in df.index that weren't included in the mapping,\n",
    "    # you can append them (here sorted alphabetically, but you can choose another order)\n",
    "    remaining_roles = [role for role in df.index if role not in ordered_roles]\n",
    "    ordered_roles.extend(sorted(remaining_roles))\n",
    "\n",
    "    # Reindex the DataFrame according to the custom order.\n",
    "    df = df.reindex(ordered_roles)\n",
    "\n",
    "\n",
    "    # First format the baseline values\n",
    "    df_formatted = format_baseline(df)\n",
    "\n",
    "    # Then apply the styling\n",
    "    df_styled = latex_color_columns(df_formatted)  # This function returns a new DataFrame with LaTeX strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe to a csv file\n",
    "#take model name\n",
    "model_name = model_path.split(\"\\\\\")[-1]\n",
    "#save the dataframe to a csv file\n",
    "df.to_csv(f\"{model_name}_ablation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_styled.to_latex(escape=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
