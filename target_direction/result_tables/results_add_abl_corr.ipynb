{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\user\\Desktop\\temp\\rolevectors_results\\gemma-2-9b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_DATASET_MAPPING = {\n",
    "    \"econ\": [\"economic researcher\", \"economist\", \"financial analyst\"],\n",
    "    \"eecs\": [\"electronics technician\", \"data scientist\", \"electrical engineer\", \"software engineer\", \"web developer\"],\n",
    "    \"law\": [\"bailiff\", \"lawyer\"],\n",
    "    \"math\": [\"data analyst\", \"mathematician\", \"statistician\"],\n",
    "    \"medicine\": [\"nurse\", \"doctor\", \"physician\", \"dentist\", \"surgeon\"],\n",
    "    \"natural science\": [\"geneticist\", \"biologist\", \"physicist\", \"teacher\", \"chemist\", \"ecologist\"],\n",
    "    \"politics\": [\"politician\", \"sheriff\", \"governor\", \"enthusiast\", \"partisan\"],\n",
    "    \"psychology\": [\"psychologist\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def format_baseline(df):\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_formatted = df.copy()\n",
    "    \n",
    "    # For each dataset in the MultiIndex (first level of columns)\n",
    "    for dataset in df.columns.levels[0]:\n",
    "        if 'baseline' in df[dataset].columns:\n",
    "            # Format baseline values to always show 2 decimal places\n",
    "            df_formatted[(dataset, 'baseline')] = df[dataset]['baseline'].apply(lambda x: f\"{float(x):.2f}\")\n",
    "    \n",
    "    return df_formatted\n",
    "\n",
    "def color_columns(df):\n",
    "    def color_value(val, baseline):\n",
    "        # Convert values to floats if needed\n",
    "        if isinstance(val, str):\n",
    "            if '±' in val:\n",
    "                val = float(val.split('±')[0].strip())\n",
    "            else:\n",
    "                val = float(val)\n",
    "        if isinstance(baseline, str):\n",
    "            if '±' in baseline:\n",
    "                baseline = float(baseline.split('±')[0].strip())\n",
    "            else:\n",
    "                baseline = float(baseline)\n",
    "                \n",
    "        val = round(val, 2)\n",
    "        baseline = round(baseline, 2)\n",
    "        diff = val - baseline\n",
    "        \n",
    "        # Calculate relative difference (percentage)\n",
    "        rel_diff = (diff / baseline) * 100 if baseline != 0 else diff * 100\n",
    "        \n",
    "        # Use logarithmic scaling for intensity\n",
    "        intensity = np.log1p(abs(rel_diff)) / 10  # log1p is log(1+x)\n",
    "        intensity = np.clip(intensity, 0.1, 0.5)  # Clip between 0.1 and 0.5\n",
    "        \n",
    "        # Choose colors based on the magnitude of difference\n",
    "        if diff > 0:\n",
    "            if rel_diff > 5:  # Large positive difference\n",
    "                return f'background-color: rgba(0, 200, 0, {intensity}); color: black; border: 1px solid #000000'\n",
    "            else:  # Small positive difference\n",
    "                return f'background-color: rgba(144, 238, 144, {intensity}); color: black; border: 1px solid #000000'\n",
    "        else:\n",
    "            if rel_diff < -5:  # Large negative difference\n",
    "                return f'background-color: rgba(255, 0, 0, {intensity}); color: black; border: 1px solid #000000'\n",
    "            else:  # Small negative difference\n",
    "                return f'background-color: rgba(255, 182, 193, {intensity}); color: black; border: 1px solid #000000'\n",
    "\n",
    "    def style_df(x):\n",
    "        # Create an empty DataFrame for styles with the same index and columns as x\n",
    "        df_styled = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "        # Ensure all cells have at least a border style\n",
    "        df_styled = df_styled.fillna('border: 1px solid #000000')\n",
    "        \n",
    "        # Iterate through each dataset (top level of MultiIndex)\n",
    "        for dataset in x.columns.levels[0]:\n",
    "            # Check if the required columns exist: baseline and 1.0\n",
    "            if 'baseline' in x[dataset].columns and '1.0' in x[dataset].columns:\n",
    "                baseline_values = x[dataset]['baseline']\n",
    "                \n",
    "                # Style the '1.0' column based on the baseline\n",
    "                for idx in x.index:\n",
    "                    df_styled.loc[idx, (dataset, '1.0')] = color_value(\n",
    "                        x.loc[idx, (dataset, '1.0')],\n",
    "                        baseline_values[idx]\n",
    "                    )\n",
    "                    # Always set the baseline column to have a border\n",
    "                    df_styled.loc[idx, (dataset, 'baseline')] = 'border: 1px solid #000000'\n",
    "                    \n",
    "                    # If the '3.0' column exists, style it based on the baseline\n",
    "                    if '3.0' in x[dataset].columns:\n",
    "                        df_styled.loc[idx, (dataset, '3.0')] = color_value(\n",
    "                            x.loc[idx, (dataset, '3.0')],\n",
    "                            baseline_values[idx]\n",
    "                        )\n",
    "            if 'baseline' in x[dataset].columns and 'ablation' in x[dataset].columns:\n",
    "                baseline_values = x[dataset]['baseline']\n",
    "                \n",
    "                # Style the 'ablation' column based on the baseline\n",
    "                for idx in x.index:\n",
    "                    df_styled.loc[idx, (dataset, 'ablation')] = color_value(\n",
    "                        x.loc[idx, (dataset, 'ablation')],\n",
    "                        baseline_values[idx]\n",
    "                    )\n",
    "                    # Always set the baseline column to have a border\n",
    "                    df_styled.loc[idx, (dataset, 'baseline')] = 'border: 1px solid #000000'\n",
    "                    \n",
    "                    # If the '3.0' column exists, style it based on the baseline\n",
    "                    if '3.0' in x[dataset].columns:\n",
    "                        df_styled.loc[idx, (dataset, '3.0')] = color_value(\n",
    "                            x.loc[idx, (dataset, '3.0')],\n",
    "                            baseline_values[idx]\n",
    "                        )\n",
    "                    \n",
    "        return df_styled\n",
    "\n",
    "    return style_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping from dataset category to list of roles.\n",
    "ROLE_DATASET_MAPPING = {\n",
    "    \"econ\": [\"economic researcher\", \"economist\", \"financial analyst\"],\n",
    "    \"eecs\": [\"electronics technician\", \"data scientist\", \"electrical engineer\", \"software engineer\", \"web developer\"],\n",
    "    \"law\": [\"bailiff\", \"lawyer\"],\n",
    "    \"math\": [\"data analyst\", \"mathematician\", \"statistician\"],\n",
    "    \"medicine\": [\"nurse\", \"doctor\", \"physician\", \"dentist\", \"surgeon\"],\n",
    "    \"natural_science\": [\"geneticist\", \"biologist\", \"physicist\", \"teacher\", \"chemist\", \"ecologist\"],\n",
    "    \"politics\": [\"politician\", \"sheriff\", \"governor\", \"enthusiast\", \"partisan\"],\n",
    "    \"psychology\": [\"psychologist\"]\n",
    "}\n",
    "\n",
    "def get_dataset_category(role):\n",
    "    \"\"\"\n",
    "    Returns the dataset category for a given role by looking up the mapping.\n",
    "    \"\"\"\n",
    "    for category, roles in ROLE_DATASET_MAPPING.items():\n",
    "        if role in roles:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "def compute_mean_score(filepath):\n",
    "    \"\"\"\n",
    "    Given a JSON file (a list of dicts), compute the mean of the 'score' values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        scores = [d[\"score\"] for d in data if \"score\" in d]\n",
    "        return round(np.mean(scores), 2) if scores else np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def get_baseline(select_dir):\n",
    "    \"\"\"\n",
    "    In the run version folder (e.g. \"1.0/select_direction\"),\n",
    "    load results_baseline.json and return its mean score.\n",
    "    \"\"\"\n",
    "    baseline_file = os.path.join(select_dir, \"results_baseline.json\")\n",
    "    if os.path.exists(baseline_file):\n",
    "        return compute_mean_score(baseline_file)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_addition_scores(select_dir, layer_range=False, start_percent=50):\n",
    "    \"\"\"\n",
    "    Iterate over coefficient subdirectories under select_dir and compute scores.\n",
    "    \"\"\"\n",
    "    addition_scores = []\n",
    "    for sub in os.listdir(select_dir):\n",
    "        sub_path = os.path.join(select_dir, sub)\n",
    "        if os.path.isdir(sub_path) and re.match(r\"^-?\\d+$\", sub):\n",
    "            inner_dirs = [\n",
    "                d for d in os.listdir(sub_path)\n",
    "                if os.path.isdir(os.path.join(sub_path, d)) and re.match(r\"^\\d+$\", d)\n",
    "            ]\n",
    "            inner_dirs_sorted = sorted(inner_dirs, key=lambda x: int(x))\n",
    "            n_layers = len(inner_dirs_sorted)\n",
    "            if n_layers == 0:\n",
    "                continue\n",
    "            if layer_range:\n",
    "                start_idx = int(np.floor(n_layers * (start_percent / 100)))\n",
    "                end_idx = int(np.ceil(n_layers * 0.8))\n",
    "                start_idx = min(start_idx, end_idx - 1)\n",
    "                selected_dirs = inner_dirs_sorted[start_idx:end_idx]\n",
    "            else:\n",
    "                keep_count = max(1, int(np.ceil(n_layers * 0.8)))\n",
    "                selected_dirs = inner_dirs_sorted[:keep_count]\n",
    "            \n",
    "            for inner in selected_dirs:\n",
    "                inner_path = os.path.join(sub_path, inner)\n",
    "                for file in os.listdir(inner_path):\n",
    "                    if file.startswith(\"results_addition\") and file.endswith(\".json\"):\n",
    "                        file_path = os.path.join(inner_path, file)\n",
    "                        mean_file_score = compute_mean_score(file_path)\n",
    "                        if not np.isnan(mean_file_score):\n",
    "                            addition_scores.append(mean_file_score)\n",
    "    \n",
    "    if addition_scores:\n",
    "        return np.mean(addition_scores), np.std(addition_scores)\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def get_select_best_addition_score(version_path):\n",
    "    \"\"\"\n",
    "    Get addition score for select_best mode by reading the metadata file.\n",
    "    The metadata is expected at:\n",
    "      {model}/{role}/{dataset}/{coeff}/direction_metadata.json\n",
    "    Then, the addition score is computed from:\n",
    "      {model}/{role}/{dataset}/{coeff}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(version_path, \"direction_metadata.json\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"Warning: metadata file not found in {metadata_path}\")\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metadata file {metadata_path}: {e}\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    if \"pos\" not in metadata or \"layer\" not in metadata:\n",
    "        print(f\"Warning: metadata file {metadata_path} missing 'pos' or 'layer'\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    pos = str(metadata[\"pos\"])\n",
    "    layer = str(metadata[\"layer\"])\n",
    "    select_dir = os.path.join(version_path, \"select_direction\")\n",
    "    addition_file = os.path.join(select_dir, pos, layer, f\"results_addition_{pos}_{layer}.json\")\n",
    "    if not os.path.exists(addition_file):\n",
    "        print(f\"Warning: addition file not found: {addition_file}\")\n",
    "        return np.nan, np.nan\n",
    "    score = compute_mean_score(addition_file)\n",
    "    # Since this is a single performance, we return the mean only (std remains NaN)\n",
    "    return score, np.nan\n",
    "\n",
    "def process_run_version(version_path, layer_range=False, start_percent=50, select_best=False):\n",
    "    \"\"\"\n",
    "    Process the run version and compute baseline and addition scores.\n",
    "    If select_best is True, the metadata is used to select the single best direction.\n",
    "    \"\"\"\n",
    "    select_dir = os.path.join(version_path, \"select_direction\")\n",
    "    if not os.path.exists(select_dir):\n",
    "        return (np.nan, (np.nan, np.nan))\n",
    "    \n",
    "    # Only version \"1.0\" is expected to have a baseline.\n",
    "    base_val = get_baseline(select_dir) if os.path.basename(version_path) == \"1.0\" else np.nan\n",
    "    \n",
    "    if select_best:\n",
    "        addition_mean, addition_std = get_select_best_addition_score(version_path)\n",
    "    else:\n",
    "        addition_mean, addition_std = get_addition_scores(select_dir, layer_range, start_percent)\n",
    "    \n",
    "    return base_val, (addition_mean, addition_std)\n",
    "\n",
    "def get_best_direction(role, primary_dataset_path, version, baseline):\n",
    "    \"\"\"\n",
    "    For a given role and version folder inside the primary dataset folder, load the filtered direction evaluations file\n",
    "    and return (position, layer) as strings.\n",
    "\n",
    "    Expects the JSON file at:\n",
    "       {model}/{role}/{primary_dataset}/{version}/select_direction/direction_evaluations_filtered.json\n",
    "\n",
    "    When version is \"3.0\", since the file in that folder doesn't contain valid ablation information,\n",
    "    the ablation values are retrieved from the baseline file at:\n",
    "       {model}/{role}/{primary_dataset}/1.0/select_direction/direction_evaluations_filtered.json\n",
    "\n",
    "    For versions other than \"3.0\", the JSON file is expected to contain both steering and ablation values.\n",
    "    \n",
    "    The function first searches for an entry where:\n",
    "       - steering_performance_score >= baseline, and\n",
    "       - ablation_performance_score < baseline.\n",
    "       \n",
    "    If no such entry is found:\n",
    "       - For version \"3.0\": the function selects the candidate with the highest steering_performance_score from the 3.0 file.\n",
    "       - For version \"1.0\": the function relaxes the ablation condition and selects the candidate with the highest steering_performance_score.\n",
    "    \n",
    "    If a suitable entry is found, its \"position\" and \"layer\" values are returned as strings.\n",
    "    Otherwise, (None, None) is returned.\n",
    "    \"\"\"\n",
    "    if version == \"3.0\":\n",
    "        eval_file_path = os.path.join(\n",
    "            primary_dataset_path, version, \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        # Load ablation values from baseline (version \"1.0\")\n",
    "        baseline_eval_file_path = os.path.join(\n",
    "            primary_dataset_path, \"1.0\", \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(eval_file_path):\n",
    "            print(f\"Warning: filtered evaluations file not found for role '{role}' in {eval_file_path}\")\n",
    "            return None, None\n",
    "        if not os.path.exists(baseline_eval_file_path):\n",
    "            print(f\"Warning: baseline filtered evaluations file not found for role '{role}' in {baseline_eval_file_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            with open(eval_file_path, \"r\") as f:\n",
    "                eval_entries = json.load(f)\n",
    "            with open(baseline_eval_file_path, \"r\") as f:\n",
    "                baseline_entries = json.load(f)\n",
    "            \n",
    "            # Build a dictionary mapping (position, layer) to ablation_performance_score from the baseline file.\n",
    "            baseline_ablation = {}\n",
    "            for entry in baseline_entries:\n",
    "                if (\"position\" in entry and \"layer\" in entry and \n",
    "                    \"ablation_performance_score\" in entry):\n",
    "                    key = (str(entry[\"position\"]), str(entry[\"layer\"]))\n",
    "                    baseline_ablation[key] = entry[\"ablation_performance_score\"]\n",
    "            \n",
    "            # First, try to find an entry satisfying the conditions.\n",
    "            for entry in eval_entries:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    pos = str(entry[\"position\"])\n",
    "                    layer = str(entry[\"layer\"])\n",
    "                    steer_perf = entry[\"steering_performance_score\"]\n",
    "                    \n",
    "                    key = (pos, layer)\n",
    "                    if key not in baseline_ablation:\n",
    "                        continue\n",
    "                    ablation_perf = baseline_ablation[key]\n",
    "                    \n",
    "                    if steer_perf >= baseline and ablation_perf < baseline:\n",
    "                        return pos, layer\n",
    "            \n",
    "            # If no candidate meets the conditions, select the best candidate from the 3.0 evaluations\n",
    "            best_entry = None\n",
    "            best_score = -float(\"inf\")\n",
    "            for entry in eval_entries:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    score = entry[\"steering_performance_score\"]\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_entry = entry\n",
    "            \n",
    "            if best_entry is not None:\n",
    "                return str(best_entry[\"position\"]), str(best_entry[\"layer\"])\n",
    "            else:\n",
    "                print(f\"Warning: No valid entries found in {eval_file_path}.\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading evaluations for role '{role}' in version '3.0': {e}\")\n",
    "            return None, None\n",
    "\n",
    "    else:\n",
    "        # For versions other than \"3.0\"\n",
    "        eval_file_path = os.path.join(\n",
    "            primary_dataset_path, version, \"select_direction\", \"direction_evaluations_filtered.json\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(eval_file_path):\n",
    "            print(f\"Warning: filtered evaluations file not found for role '{role}' in {eval_file_path}\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            with open(eval_file_path, \"r\") as f:\n",
    "                evaluations = json.load(f)\n",
    "                \n",
    "            # First, try to find an entry satisfying the conditions.\n",
    "            for entry in evaluations:\n",
    "                if (\"steering_performance_score\" in entry and \n",
    "                    \"ablation_performance_score\" in entry and\n",
    "                    \"position\" in entry and \n",
    "                    \"layer\" in entry):\n",
    "                    \n",
    "                    if (entry[\"steering_performance_score\"] >= baseline and \n",
    "                        entry[\"ablation_performance_score\"] < baseline):\n",
    "                        return str(entry[\"position\"]), str(entry[\"layer\"])\n",
    "            \n",
    "            # For version \"1.0\", relax the ablation requirement if no candidate is found.\n",
    "            if version == \"1.0\":\n",
    "                best_entry = None\n",
    "                best_score = -float(\"inf\")\n",
    "                for entry in evaluations:\n",
    "                    if (\"steering_performance_score\" in entry and \n",
    "                        \"position\" in entry and \n",
    "                        \"layer\" in entry):\n",
    "                        score = entry[\"steering_performance_score\"]\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_entry = entry\n",
    "                if best_entry is not None:\n",
    "                    return str(best_entry[\"position\"]), str(best_entry[\"layer\"])\n",
    "            \n",
    "            print(f\"Warning: No evaluation entry in {eval_file_path} satisfies the conditions.\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading evaluations for role '{role}' in {eval_file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "def get_score_for_dataset_with_direction(model_name, role, dataset, version, pos, layer):\n",
    "    \"\"\"\n",
    "    For a given role, dataset, and version (coefficient) folder, load the results file using the provided\n",
    "    best direction parameters.\n",
    "    \n",
    "    The expected file path is:\n",
    "       {model_name}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(model_name, role, dataset, version, \"select_direction\", pos, layer,\n",
    "                             f\"results_addition_{pos}_{layer}.json\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: score file not found: {file_path}\")\n",
    "        return np.nan\n",
    "    return compute_mean_score(file_path)\n",
    "\n",
    "def get_score_for_dataset_with_direction_ablation(model_name, role, dataset, version, pos, layer):\n",
    "    \"\"\"\n",
    "    For a given role, dataset, and version (coefficient) folder, load the results file using the provided\n",
    "    best direction parameters.\n",
    "    \n",
    "    The expected file path is:\n",
    "       {model_name}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_ablation_{pos}_{layer}.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(model_name, role, dataset, version, \"select_direction\", pos, layer,\n",
    "                             f\"results_ablation_{pos}_{layer}.json\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: score file not found: {file_path}\")\n",
    "        return np.nan\n",
    "    return compute_mean_score(file_path)\n",
    "\n",
    "def main(model_name, layer_range=False, start_percent=50, select_best=False, select_best_role=False, select_best_role_ablation=False):\n",
    "    \"\"\"\n",
    "    Builds a MultiIndex DataFrame reporting performance scores.\n",
    "    \n",
    "    When select_best_role is True, for each role the code:\n",
    "      1. Determines the \"primary\" dataset via get_dataset_category().\n",
    "      2. In that primary folder (e.g. {model}/{role}/{primary_dataset}) identifies all version folders.\n",
    "      3. For each version, loads the best direction from its filtered evaluations file using the provided baseline.\n",
    "      4. Then for every dataset folder under {model}/{role} (excluding folders like \"generate_directions\" or \"test_direction\"),\n",
    "         if that folder contains the same version folder, the score is loaded from:\n",
    "              {model}/{role}/{dataset}/{version}/select_direction/{pos}/{layer}/results_addition_{pos}_{layer}.json\n",
    "      5. For version \"1.0\", the score is also recorded as the baseline.\n",
    "    \n",
    "    The DataFrame’s rows are roles and its columns use a MultiIndex with:\n",
    "      - Level 0: dataset folder name (e.g. \"law\", \"math\", …)\n",
    "      - Level 1: version (with \"baseline\" included for version \"1.0\")\n",
    "    \n",
    "    (When select_best_role is False, the original logic is used; that branch is not implemented here.)\n",
    "    \"\"\"\n",
    "    if sum([bool(layer_range), bool(select_best), bool(select_best_role), bool(select_best_role_ablation)]) > 1:\n",
    "         raise ValueError(\"Only one of layer_range, select_best, select_best_role, or select_best_role_ablation can be True.\")\n",
    "    \n",
    "    if select_best_role:\n",
    "         roles = [d for d in os.listdir(model_name) if os.path.isdir(os.path.join(model_name, d))]\n",
    "         results = {}\n",
    "         for role in roles:\n",
    "              role_path = os.path.join(model_name, role)\n",
    "              primary_dataset = get_dataset_category(role)\n",
    "              if primary_dataset is None:\n",
    "                  print(f\"Warning: No primary dataset mapping for role '{role}'. Skipping best-direction evaluation for this role.\")\n",
    "                  continue\n",
    "              primary_dataset_path = os.path.join(role_path, primary_dataset)\n",
    "              if not os.path.exists(primary_dataset_path):\n",
    "                  print(f\"Warning: Primary dataset folder '{primary_dataset_path}' not found for role '{role}'. Skipping.\")\n",
    "                  continue\n",
    "              # Compute baseline from the primary dataset's version \"1.0\" select_direction folder.\n",
    "              baseline_select_dir = os.path.join(primary_dataset_path, \"1.0\", \"select_direction\")\n",
    "              baseline_value = get_baseline(baseline_select_dir)\n",
    "              # Get the list of version folders (e.g. \"1.0\", \"3.0\", etc.) from the primary dataset folder.\n",
    "              primary_versions = [v for v in os.listdir(primary_dataset_path) if re.match(r\"^\\d+\\.\\d+$\", v)]\n",
    "              primary_versions = sorted(primary_versions, key=lambda x: float(x))\n",
    "              # Get all dataset directories under the role (excluding known non-performance folders).\n",
    "              all_datasets = [d for d in os.listdir(role_path) \n",
    "                              if os.path.isdir(os.path.join(role_path, d)) \n",
    "                              and d not in [\"generate_directions\", \"test_direction\"]]\n",
    "              # Prepare a dictionary: for each dataset, a dict mapping version (and baseline) to score.\n",
    "              ds_result = {ds: {} for ds in all_datasets}\n",
    "              for ver in primary_versions:\n",
    "                   # Get best direction using the new function that requires baseline.\n",
    "                   pos, layer = get_best_direction(role, primary_dataset_path, ver, baseline_value)\n",
    "                   if pos is None or layer is None:\n",
    "                        print(f\"Skipping version {ver} for role '{role}' due to missing best direction.\")\n",
    "                        continue\n",
    "                   # For each dataset folder, if it contains the same version folder, load its score.\n",
    "                   for ds in all_datasets:\n",
    "                        ds_ver_path = os.path.join(role_path, ds, ver)\n",
    "                        if os.path.exists(ds_ver_path):\n",
    "                            score = get_score_for_dataset_with_direction(model_name, role, ds, ver, pos, layer)\n",
    "                            ds_result[ds][ver] = score if score != \"nan\" else \"nan\"\n",
    "                        else:\n",
    "                            ds_result[ds][ver] = \"nan\"\n",
    "                        # If this is the \"1.0\" version, also record its baseline.\n",
    "                        if ver == \"1.0\":\n",
    "                            select_dir = os.path.join(model_name, role, ds, ver, \"select_direction\")\n",
    "                            baseline_score = get_baseline(select_dir)\n",
    "                            ds_result[ds][\"baseline\"] = f\"{baseline_score:.2f}\" if not np.isnan(baseline_score) else \"nan\"\n",
    "              results[role] = ds_result\n",
    "         \n",
    "         # Build a DataFrame from the results dictionary.\n",
    "         all_dataset_set = set()\n",
    "         all_version_set = set()\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   all_dataset_set.add(ds)\n",
    "                   all_version_set.update(ver_dict.keys())\n",
    "         all_dataset_list = sorted(all_dataset_set)\n",
    "         # Sort versions so that \"baseline\" always comes first, then numeric order.\n",
    "         def sort_ver(x):\n",
    "              if x == \"baseline\":\n",
    "                   return (0, 0)\n",
    "              try:\n",
    "                   return (1, float(x))\n",
    "              except:\n",
    "                   return (1, x)\n",
    "         all_version_list = sorted(list(all_version_set), key=sort_ver)\n",
    "         cols = pd.MultiIndex.from_product([all_dataset_list, all_version_list], names=[\"Dataset\", \"Version\"])\n",
    "         df = pd.DataFrame(index=sorted(results.keys()), columns=cols)\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   for ver, value in ver_dict.items():\n",
    "                        df.loc[role, (ds, ver)] = value\n",
    "         return df\n",
    "\n",
    "    elif select_best_role_ablation:\n",
    "         roles = [d for d in os.listdir(model_name) if os.path.isdir(os.path.join(model_name, d))]\n",
    "         results = {}\n",
    "         for role in roles:\n",
    "              role_path = os.path.join(model_name, role)\n",
    "              primary_dataset = get_dataset_category(role)\n",
    "              if primary_dataset is None:\n",
    "                  print(f\"Warning: No primary dataset mapping for role '{role}'. Skipping best-direction evaluation for this role.\")\n",
    "                  continue\n",
    "              primary_dataset_path = os.path.join(role_path, primary_dataset)\n",
    "              if not os.path.exists(primary_dataset_path):\n",
    "                  print(f\"Warning: Primary dataset folder '{primary_dataset_path}' not found for role '{role}'. Skipping.\")\n",
    "                  continue\n",
    "              # Compute baseline from the primary dataset's version \"1.0\" select_direction folder.\n",
    "              baseline_select_dir = os.path.join(primary_dataset_path, \"1.0\", \"select_direction\")\n",
    "              baseline_value = get_baseline(baseline_select_dir)\n",
    "              # Get the list of version folders from the primary dataset folder.\n",
    "              primary_versions = [v for v in os.listdir(primary_dataset_path) if re.match(r\"^\\d+\\.\\d+$\", v)]\n",
    "              primary_versions = sorted(primary_versions, key=lambda x: float(x))\n",
    "              # Get all dataset directories under the role (excluding known non-performance folders).\n",
    "              all_datasets = [d for d in os.listdir(role_path) \n",
    "                              if os.path.isdir(os.path.join(role_path, d)) \n",
    "                              and d not in [\"generate_directions\", \"test_direction\"]]\n",
    "              # Prepare a dictionary: for each dataset, a dict mapping version (and baseline) to score.\n",
    "              ds_result = {ds: {} for ds in all_datasets}\n",
    "              for ver in primary_versions:\n",
    "                   if ver == \"3.0\":\n",
    "                       continue\n",
    "                   # Get best direction using baseline_value.\n",
    "                   pos, layer = get_best_direction(role, primary_dataset_path, ver, baseline_value)\n",
    "                   if pos is None or layer is None:\n",
    "                        print(f\"Skipping version {ver} for role '{role}' due to missing best direction.\")\n",
    "                        continue\n",
    "                   # For each dataset folder, if it contains the same version folder, load its ablation score.\n",
    "                   for ds in all_datasets:\n",
    "                        ds_ver_path = os.path.join(role_path, ds, ver)\n",
    "                        if os.path.exists(ds_ver_path):\n",
    "                            score = get_score_for_dataset_with_direction_ablation(model_name, role, ds, ver, pos, layer)\n",
    "                            ds_result[ds][\"ablation\"] = score if score != \"nan\" else \"nan\"\n",
    "                        else:\n",
    "                            ds_result[ds][\"ablation\"] = \"nan\"\n",
    "                        # If this is the \"1.0\" version, also record its baseline.\n",
    "                        if ver == \"1.0\":\n",
    "                            select_dir = os.path.join(model_name, role, ds, ver, \"select_direction\")\n",
    "                            baseline_score = get_baseline(select_dir)\n",
    "                            ds_result[ds][\"baseline\"] = f\"{baseline_score:.2f}\" if not np.isnan(baseline_score) else \"nan\"\n",
    "              results[role] = ds_result\n",
    "         \n",
    "         # Build a DataFrame from the results dictionary.\n",
    "         all_dataset_set = set()\n",
    "         all_version_set = set()\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   all_dataset_set.add(ds)\n",
    "                   all_version_set.update(ver_dict.keys())\n",
    "         all_dataset_list = sorted(all_dataset_set)\n",
    "         # Sort versions so that \"baseline\" always comes first, then numeric order.\n",
    "         def sort_ver(x):\n",
    "              if x == \"baseline\":\n",
    "                   return (0, 0)\n",
    "              try:\n",
    "                   return (1, float(x))\n",
    "              except:\n",
    "                   return (1, x)\n",
    "         all_version_list = sorted(list(all_version_set), key=sort_ver)\n",
    "         cols = pd.MultiIndex.from_product([all_dataset_list, all_version_list], names=[\"Dataset\", \"Version\"])\n",
    "         df = pd.DataFrame(index=sorted(results.keys()), columns=cols)\n",
    "         for role, ds_dict in results.items():\n",
    "              for ds, ver_dict in ds_dict.items():\n",
    "                   for ver, value in ver_dict.items():\n",
    "                        df.loc[role, (ds, ver)] = value\n",
    "         return df\n",
    "\n",
    "    else:\n",
    "         raise NotImplementedError(\"Other modes (non-select_best_role) are not implemented in this snippet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def transform_multiindex_df(df):\n",
    "    \"\"\"\n",
    "    Given a multi-index DataFrame with columns like (Dataset, Version)\n",
    "    where each dataset contains a \"baseline\" column as well as one or more version columns\n",
    "    (e.g., \"1.0\", \"3.0\", \"ablation\"), this function computes:\n",
    "         diff = (baseline - version_score) / baseline * 100\n",
    "    for each version (excluding the baseline). It then flattens the differences into a single Series.\n",
    "    \n",
    "    This version converts the string values to numeric types before doing the subtraction.\n",
    "    \"\"\"\n",
    "    diff_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Process each dataset in the multi-index columns\n",
    "    for ds in df.columns.get_level_values(0).unique():\n",
    "        if \"baseline\" in df[ds].columns:\n",
    "            # Convert the baseline column to numeric values.\n",
    "            baseline = pd.to_numeric(df[(ds, \"baseline\")], errors=\"coerce\")\n",
    "            # Loop over all version columns that are not \"baseline\"\n",
    "            for ver in df[ds].columns:\n",
    "                if ver == \"baseline\":\n",
    "                    continue\n",
    "                ver_series = pd.to_numeric(df[(ds, ver)], errors=\"coerce\")\n",
    "                col_name = f\"{ds}_{ver}\"\n",
    "                diff_df[col_name] = ver_series.sub(baseline).div(baseline).mul(100)\n",
    "        else:\n",
    "            print(f\"Dataset {ds} does not have a baseline column; skipping difference computation for it.\")\n",
    "    \n",
    "    # Flatten the diff_df: iterate over each row and append all computed differences\n",
    "    flat_list = []\n",
    "    for i in diff_df.index:\n",
    "        for col in diff_df.columns:\n",
    "            flat_list.append(diff_df.loc[i, col])\n",
    "    flattened_series = pd.Series(flat_list)\n",
    "    return flattened_series\n",
    "\n",
    "\n",
    "\n",
    "def create_models_matrix(models_series):\n",
    "    \"\"\"\n",
    "    Given a dictionary mapping model names to flattened Series (vectors of differences),\n",
    "    create a DataFrame whose columns are the model names and rows correspond to the\n",
    "    concatenated (flattened) differences. If the lengths differ, shorter arrays are padded with NaN.\n",
    "    \"\"\"\n",
    "    max_len = max(len(s) for s in models_series.values())\n",
    "    data = {}\n",
    "    for model, series in models_series.items():\n",
    "        arr = series.values\n",
    "        if len(arr) < max_len:\n",
    "            # Pad with NaN at the end if necessary.\n",
    "            arr = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
    "        data[model] = arr\n",
    "    matrix_df = pd.DataFrame(data)\n",
    "    return matrix_df\n",
    "\n",
    "\n",
    "# Replace with the parent folder that contains your models.\n",
    "models_parent_dir = r\"C:\\Users\\user\\Desktop\\temp\\rolevectors_results\"\n",
    "# Get a list of model directories.\n",
    "models = [d for d in os.listdir(models_parent_dir) if os.path.isdir(os.path.join(models_parent_dir, d))]\n",
    "\n",
    "all_model_series = {}\n",
    "\n",
    "for model in models:\n",
    "    model_path = os.path.join(models_parent_dir, model)\n",
    "    try:\n",
    "        # Call your existing main() function.\n",
    "        # (Assuming that your main() function returns a multi-index DataFrame.\n",
    "        #  Here we use select_best_role_ablation=True as an example.)\n",
    "        df_multi = main(model_path, select_best_role=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping model {model} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Transform the multi-index DataFrame: compute (baseline - score) differences and flatten.\n",
    "    diff_series = transform_multiindex_df(df_multi)\n",
    "    all_model_series[model] = diff_series\n",
    "\n",
    "# Now, create the final matrix: each column is the flattened differences for one model.\n",
    "final_matrix = create_models_matrix(all_model_series)\n",
    "\n",
    "# For example, print or save the matrix:\n",
    "print(final_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assume that final_matrix is a DataFrame where each column represents a model.\n",
    "# final_matrix = pd.DataFrame({...})\n",
    "\n",
    "# 1. Change the order of models.\n",
    "new_order = [\n",
    "    'gemma-2-2b-it', \n",
    "    'gemma-2-9b-it',\n",
    "    'Llama-3.2-1B-Instruct', \n",
    "    'Llama-3.2-3B-Instruct', \n",
    "    'Llama-3.1-8B-Instruct', \n",
    "    'Qwen-1_8B-Chat', \n",
    "    'Qwen-7B-Chat',   \n",
    "]\n",
    "final_matrix = final_matrix[new_order]\n",
    "final_matrix = final_matrix.rename(columns={'Qwen-1_8B-Chat': 'Qwen-1.8B'})\n",
    "final_matrix = final_matrix.rename(columns={'gemma-2-2b-it': 'gemma-2-2b'})\n",
    "final_matrix = final_matrix.rename(columns={'gemma-2-9b-it': 'gemma-2-9b'})\n",
    "final_matrix = final_matrix.rename(columns={'Llama-3.2-1B-Instruct': 'Llama-3.2-1B'})\n",
    "final_matrix = final_matrix.rename(columns={'Llama-3.2-3B-Instruct': 'Llama-3.2-3B'})\n",
    "final_matrix = final_matrix.rename(columns={'Llama-3.1-8B-Instruct': 'Llama-3.1-8B'})\n",
    "final_matrix = final_matrix.rename(columns={'Qwen-7B-Chat': 'Qwen-7B'})\n",
    "\n",
    "# Update the list of models\n",
    "models = final_matrix.columns\n",
    "\n",
    "# 2. Compute the correlation matrix.\n",
    "corr_matrix = final_matrix.corr()\n",
    "\n",
    "# 3. Compute p-values for each correlation pair.\n",
    "p_values = pd.DataFrame(index=models, columns=models, dtype=float)\n",
    "\n",
    "for i in models:\n",
    "    for j in models:\n",
    "        x = final_matrix[i]\n",
    "        y = final_matrix[j]\n",
    "        df_pair = pd.concat([x, y], axis=1).dropna()\n",
    "        if df_pair.shape[0] > 1:\n",
    "            r, p = pearsonr(df_pair.iloc[:, 0], df_pair.iloc[:, 1])\n",
    "        else:\n",
    "            p = np.nan\n",
    "        p_values.loc[i, j] = p\n",
    "\n",
    "# 4. Create an annotation matrix.\n",
    "def significance_stars(p):\n",
    "    if pd.isna(p):\n",
    "        return \"\"\n",
    "    elif p < 0.001:\n",
    "        return \"***\"\n",
    "    elif p < 0.01:\n",
    "        return \"**\"\n",
    "    elif p < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "annot = corr_matrix.copy().astype(str)\n",
    "for i in models:\n",
    "    for j in models:\n",
    "        star = significance_stars(p_values.loc[i, j])\n",
    "        annot.loc[i, j] = f\"{corr_matrix.loc[i, j]:.2f}{star}\"\n",
    "\n",
    "# 5. Define a symmetric color map.\n",
    "cmap_sym = mcolors.LinearSegmentedColormap.from_list('red_white_blue', ['white', 'blue'])\n",
    "\n",
    "# Create a mask for the upper triangle.\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "annot_masked = annot.where(~mask, \"\")\n",
    "\n",
    "# Set the font scale and seaborn style (white background).\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# 6. Display the heatmap without grid and with a white background.\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap=cmap_sym,\n",
    "    vmin=-0.1,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    annot=annot_masked,\n",
    "    fmt=\"\",\n",
    "    mask=mask,\n",
    "    cbar=False,\n",
    "    annot_kws={\"size\": 16},\n",
    "    linewidths=0  # Remove cell lines \n",
    ")\n",
    "\n",
    "# Ensure the axes background is white.\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.grid(False)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
